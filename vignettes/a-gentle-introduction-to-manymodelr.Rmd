---
author: "Nelson Gonzabato"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette

vignette: >
  %\VignetteIndexEntry{ "A Gentle Introduction to manymodelr"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# manymodelr 0.2.3

In this vignette, we take a look at how we can simplify many machine learning tasks using `manymodelr`. We will take a look at the core functions first.

## Installing the package

```{r}
#install.packages("manymodelr")
```

Once the package has been successfully installed, we can then proceed by loading the package and exploring some of the key functions.



### Loading the package

```{r}
library(manymodelr)
```

#### A look at some of the key functions.

- **`agg_by_group`**

As one can guess from the name, this function provides an easy way to manipulate grouped data. We can for instance find the number of observations in the iris data set. The formula takes the form `x~y` where `y` is the grouping variable(in this case `Species`). One can supply a formula as shown next. 

```{r}
agg_by_group(iris,.~Species,length)
```

```{r}
head(agg_by_group(mtcars,cyl~hp+vs,sum))
```


- **`multi_model_1`**

This is one of the core functions of the package. Since the function uses `caret` backend, we need to load `caret` before we can use it. To avoid, several messages showing up, we use the function `suppressMessages`. This assumes that one is familiar with machine learning basics. We specify our model types and we use the argument `valid=TRUE` to specify that we are dealing with validation. Had we wanted to predict on unseen test data, then this argument would be set to `FALSE`.

```{r}
suppressMessages(library(caret))
set.seed(520)
train_set<-createDataPartition(iris$Species,p=0.8,list=FALSE)
valid_set<-iris[-train_set,]
train_set<-iris[train_set,]
ctrl<-trainControl(method="cv",number=5)
m<-multi_model_1(train_set,"Species",".",c("knn","rpart"),
"Accuracy",ctrl,newdata =valid_set,valid=TRUE)
```

The above message tells us that the model has returned our metrics for each of the model types we specified. These can be extracted as shown below. Other return values include predictions and a summary of the model.

```{r}
m$Metrics
```
```{r}
head(m$Predictions)
```


- **`fit_model`**

 Yet another core function, this allows us to fit any kind of model. It replaces `modeleR` which had several issues and development was discontinued. It can still work with some inaccuracies.
 
```{r}
iris1 <- iris[1:50,]
iris2 <- iris[51:100,]
lm_model <- fit_model(iris1,"Sepal.Length","Petal.Length","lm")
lm_model

```

To extract information about the model, we can use `extract_model_info` as follows. Say we wanted to extract the R squared, we could proceed as follows:
```{r}
extract_model_info(lm_model, "r.squared")
```

To extract the adjusted r squared,  we can do the following:
```{r}
extract_model_info(lm_model, "adj.r.squared")
```

For the p value:
```{r}
extract_model_info(lm_model, "p_value")
```

This is not restricted to linear models but will work for model types. See `help(extract_model_info)` to see currently supported model types.

To add predictions to our data set, we can use `add_model_predictions` as follows:

```{r}
# select only column 6 that has our predicted values
head(add_model_predictions(lm_model, old_data = iris1, 
                           new_data =  iris2))[6]
```

To do the same with `dplyr`, one can work as follows:

```{r}
library(dplyr)
iris1 %>% 
  add_model_predictions(model=lm_model,new_data = iris2) %>% 
  select(predicted, everything()) %>% 
  head()
```

To add residuals to our data set, we can use `add_model_residuals`:

```{r}
head(add_model_residuals(lm_model, iris1)[6])
```

With `dplyr`:

```{r}
iris1 %>% 
  add_model_residuals(model=lm_model) %>% 
  add_model_predictions(new_data = iris2, model = lm_model) %>% 
  select(predicted,residuals, everything()) %>% 
  head()
```



- **`get_var_corr`**
 
 As the name suggests, this function is useful when carrying out correlation tests as shown below. Setting `get_all` to `TRUE` implies that all the variables are correlated(from exploratory data analysis) and you just want to see what the correlation(s) is(are). The variant function `get_var_corr_` (note the underscore at the end provides a convenient way to get correlations for combinations of variables(pairs).)
 
 
```{r}
get_var_corr(mtcars, "mpg",get_all = TRUE)


```

To drop non-numeric columns, one can use the argument `drop_columns` which defaults to `TRUE` for version 0.2.3 onwards. 
```{r}
# prevent warning that informs you of removal of non numeric columns
suppressWarnings(get_var_corr(iris,comparison_var = "Sepal.Length",get_all = TRUE, drop_columns = TRUE))
```


To get correlations for only select variables, one could work as follows:

```{r}
get_var_corr(mtcars,comparison_var = "cyl",
             other_vars = c("disp","mpg"),get_all = FALSE)
```



Similarly, `get_var_corr_` (note the underscore at the end) provides  a convenient way to get combination-wise correlations.

```{r}
head(get_var_corr_(mtcars),6)

```

To use only a subset of the data, we can use the argument `subset_df`. This is logical. Once set to `TRUE`, we can then provide a `list` of `subset_cols`. By default, the first value(vector) in the list is mapped to `Comparison_Var` and the other to `Other_Var`. The list is therefore of length 2.

```{r}
get_var_corr_(mtcars,
             subset_df = TRUE,
             subset_cols = list(c("mpg","vs"),
                                c("disp","wt")),
             method="spearman",exact=FALSE)
```


- `plot_corr`

Obtaining correlations would mostly likely benefit from some form of visualization. `plot_corr` aims to achieve just that. There are currently two plot styles that is `squares` and `circles`. `circles` has the `shape` argument that can allow for more flexibility with respect to shape. It should be noted that the correlation matrix supplied to this function is an object produced by `get_var_corr_`.


```{r}
# Use defaults.
#res <- get_var_corr_(mtcars)
#plot_corr(res)
```


To modify the plot a bit, we can choose to switch the x and y values as shown below.

```{r}
#plot_corr(res, x="Other_Var", y="Comparison_Var")
```


To show significance of the results instead of the correlations themselves, we can set `show_corr` to `FALSE` and set `show_signif` to `TRUE` as shown below. By default,
significance is set to 0.05. You can override this by supplying a different `signif_cutoff` to the function.

```{r}
# color by p value
# change custom colors by supplying custom_cols
# significance is default 
#plot_corr(res, x="Other_Var", y="Comparison_Var",
 #         plot_style = "squares",
  #        show_corr = FALSE, show_signif = TRUE,
   #       colour_by = "p.value",
    #      custom_cols = c("blue","yellow","red"))
```

```{r}
#plot_corr(res, x="Other_Var", y="Comparison_Var",
 #         plot_style = "squares",
  #        show_corr = FALSE, show_signif = TRUE,
   #       colour_by = "p.value",
    #      custom_cols = c("blue","yellow","red"),
     #     signif_cutoff = 0.01)
```

To explore more options, please take a look at the documentation.


- `rowdiff`

 This is useful when trying to find differences between rows. The `direction` argument specifies how the subtractions are made while the `exclude` argument is currently used to
 remove non-numeric data.  Using `direction="reverse"` performs a subtraction akin to `x-(x-1)` where `x` is the row number. 

```{r}
head(rowdiff(iris,exclude = "non_numeric",direction = "reverse"))
```


- `na_replace`

This allows the user to conveniently replace missing values. Current options are `ffill` which replaces with the next non-missing value, `samples` that samples the data and does replacement, `value` that allows one to fill `NA`s with a specific value. Other common mathematical methods like `min`, `max`,`get_mode`, `sd`, etc are also supported.
```{r}
head(na_replace(airquality, how="value", value="Missing"),8)
```

- `na_replace_grouped`

This provides a convenient way to replace values by group.
```{r}
test_df <- data.frame(A=c(NA,1,2,3), B=c(1,5,6,NA),
                      groups=c("A","A","B","B"))
# Replace NAs by group
#agg_by_group(test_df,.~groups,mean)
# from the above we see means are [1,5] [2,6]
na_replace_grouped(df=test_df,group_by_cols = "groups",
                   how="mean")
```


##### Exploring Further.

The vignette has been short and therefore is non exhaustive. The best way to explore this and any package or language is to practice. For more examples, please use `?function_name` and see a few implementations of the given function.

###### Reporting Issues

If you would like to contribute, report issues or improve any of these functions, please raise a pull request at ([manymodelr](https://www.github.com/Nelson-Gon/manymodelr))

> "Programs must be written for people to read, and only incidentally for machines to execute." - Harold Abelson ([Reference](https://www.goodreads.com/quotes/tag/programming))

**Thank You**
